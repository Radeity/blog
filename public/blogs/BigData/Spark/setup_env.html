<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Hadoop + Spark 完全分布式学习环境搭建（Docker版） | Radeity&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/blog/favicon.ico">
    <meta name="description" content="技术 生活 一点点小记录">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.8be25012.css" as="style"><link rel="preload" href="/blog/assets/js/app.137d6028.js" as="script"><link rel="preload" href="/blog/assets/js/3.305110ea.js" as="script"><link rel="preload" href="/blog/assets/js/1.3ef5e73d.js" as="script"><link rel="preload" href="/blog/assets/js/7.3005c2a8.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.1cfb198e.js"><link rel="prefetch" href="/blog/assets/js/11.3a337f37.js"><link rel="prefetch" href="/blog/assets/js/12.fa57a69c.js"><link rel="prefetch" href="/blog/assets/js/13.c44b39c5.js"><link rel="prefetch" href="/blog/assets/js/14.2952e352.js"><link rel="prefetch" href="/blog/assets/js/15.34c8c00c.js"><link rel="prefetch" href="/blog/assets/js/16.6d622d91.js"><link rel="prefetch" href="/blog/assets/js/17.00f60129.js"><link rel="prefetch" href="/blog/assets/js/18.375cb97e.js"><link rel="prefetch" href="/blog/assets/js/19.2cf3bc5f.js"><link rel="prefetch" href="/blog/assets/js/20.9f392430.js"><link rel="prefetch" href="/blog/assets/js/21.4946bb4a.js"><link rel="prefetch" href="/blog/assets/js/22.8f59e822.js"><link rel="prefetch" href="/blog/assets/js/23.4f7c9f07.js"><link rel="prefetch" href="/blog/assets/js/4.8c99c1aa.js"><link rel="prefetch" href="/blog/assets/js/5.a88a2f5a.js"><link rel="prefetch" href="/blog/assets/js/6.319eb732.js"><link rel="prefetch" href="/blog/assets/js/8.b7837282.js"><link rel="prefetch" href="/blog/assets/js/9.b38a71f9.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.8be25012.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Radeity's Blog</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>技术 生活 一点点小记录</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Aaron_Wang</span>
            
          <span data-v-4e82dffc>2020 - </span>
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Radeity's Blog" class="logo"> <span class="site-name">Radeity's Blog</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/大数据/" class="nav-link"><i class="undefined"></i>
  大数据
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/计算机基础/" class="nav-link"><i class="undefined"></i>
  计算机基础
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/Radeity" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    Aaron_Wang
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>12</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>5</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/大数据/" class="nav-link"><i class="undefined"></i>
  大数据
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/计算机基础/" class="nav-link"><i class="undefined"></i>
  计算机基础
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/Radeity" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Hadoop + Spark 完全分布式学习环境搭建（Docker版）</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Aaron_Wang</span>
            
          <span data-v-4e82dffc>2020 - </span>
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">Hadoop + Spark 完全分布式学习环境搭建（Docker版）</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>Aaron_Wang</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>8/21/2021</span></i> <i class="iconfont reco-eye" data-v-1ff7123e><span id="/blog/blogs/BigData/Spark/setup_env.html" data-flag-title="Your Article Title" class="leancloud-visitors" data-v-1ff7123e><a class="leancloud-visitors-count" style="font-size:.9rem;font-weight:normal;color:#999;"></a></span></i> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>Spark</span></i></div></div> <div class="theme-reco-content content__default"><p><img src="/blog/assets/img/image-20210820225419279.75c4d8f2.png" alt="image-20210820225419279"></p> <p>计算机中所有程序都要寄托一个环境运行，环境可以理解为一个程序运行所需要的条件的集合；如果只是为了写一个Java程序，它是单进程的，那么我们配置jdk、jre就可以了；如果写一个网站，有前后端、数据库服务、缓存服务等等，它们都要跑在不同的端口乃至不同的服务器上，配置环境让他们可以协同运作就会变得稍微复杂一些了；本篇文章将会介绍一个同样复杂的环境配置 —— Spark + Hadoop。</p> <p>由于 Spark和Hadoop通常都运行于<strong>分布式环境</strong>，有主从节点，因此一般在Linux服务器集群进行分布式生产环境部署，但很多人在学习时可能是没有多台服务器可以练手的，这时通常会选择用虚拟化的技术实现分布式，这里的选择就包括了<strong>虚拟机</strong>和<strong>Docker</strong>。</p> <h3 id="docker-vs-虚拟机"><a href="#docker-vs-虚拟机" class="header-anchor">#</a> Docker vs 虚拟机</h3> <p>虚拟机就是用系统镜像去创建，<strong>虚拟出硬件资源</strong>直接跑一个完整的操作系统，虚拟机运行时你的电脑硬件资源被拆分后跑两个操作系统，配置分布式多节点就要跑多个完整的操作系统，想必性能受限；</p> <p>Docker是一个沙箱环境， 它在宿主机内核的基础上<strong>虚拟出了一个操作系统</strong>，更适合对不同应用的<strong>隔离</strong>，隔离程度没有虚拟机高，不用内核，没那么占据存储空间。</p> <p><img src="/blog/assets/img/IMG_7530.f961a219.png" alt="IMG_7530"></p> <p>再看看我们的需求，只需要虚拟出多个节点进行Hadoop和Spark的实验，不需要考虑真实的集群生产环境，没必要虚拟完整的操作系统，因此选择用Docker来部署本地的分布式环境。当然，更直观的一个原因是，Docker在内存和存储的消耗比虚拟机小得多，本地实验更友好。</p> <h3 id="docker-镜像"><a href="#docker-镜像" class="header-anchor">#</a> Docker 镜像</h3> <p>首先拉一个docker的centos7镜像（确保已经装了docker），发现只有200M，和虚拟机上运行的5G镜像差了20多倍，原因前面提及过，操作系统是内核加文件系统，而Docker容器运行使用的是宿主机的内核，因此Docker镜像仅包含了挂载在宿主机内核上的<strong>根文件系统</strong>，叫做<strong>Union FS</strong>，它是适配了Docker分层结构的一个文件系统。Linux中的根文件系统是文件系统特殊的一种，通常在操作系统启动阶段加载到内存中，挂载了根文件系统才能逻辑性的去访问磁盘，才能去使用基本的shell命令，才能去加载一些系统的配置文件。文件系统就是为了硬件能与用户交互产生的，因此我们在开启一个有根文件系统的容器时，也觉得像在使用一个完整的操作系统。</p> <p>镜像是只读的，容器相当于“实例化”镜像，它记录了对镜像的修改，可以在当前镜像的基础上叠加修改内容构建出一个新的镜像，由于是分层模式，一个镜像应该尽量简洁仅包含当前环境，删除掉一些无关数据；构建一个新镜像常用的两种方法是commit和Dockerfile，commit是基于当前容器构建镜像，构建出来的镜像像一个黑箱，Dockerfile是从头开始根据一行行指令去构建新镜像，逻辑更加清晰。</p> <h3 id="启动容器"><a href="#启动容器" class="header-anchor">#</a> 启动容器</h3> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ docker  <span class="token comment"># 确认Docker可以正常运行</span>
$ docker pull centos:centos7  <span class="token comment"># 拉镜像</span>
$ docker image <span class="token function">ls</span>  <span class="token comment"># 查看镜像</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>启动Docker就是启动了一个父进程Docker Daemon，Docker内启动每个容器都是启动Docker Daemon的一个子进程，子进程下有它自己的命名空间。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ docker run -it --name linux1 centos /bin/bash
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>启动容器后可以在宿主机命令行执行 docker ps 查看容器运行状态，也可以执行 docker inspect 容器ID 查看容器运行的各种相关数据。</p> <p>接下来都在容器的bash中执行命令。</p> <hr> <p>首先我们需要安装Hadoop、Spark及相关应用程序到容器中，可以wget下tar包，也可以使用 docker cp 命令将本地tar包拷贝到指定的容器目录下，解压tar包；通常用户程序我们拷贝到 /opt 中，另外还要注意彼此的版本依赖。当然，也可以在我们本地解压好直接cp到容器中。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token builtin class-name">cd</span> /opt
$ yum <span class="token function">install</span> <span class="token function">wget</span>
$ <span class="token function">wget</span> https://downloads.lightbend.com/scala/2.12.14/scala-2.12.14.tgz
$ <span class="token function">wget</span> Hadoop下载地址
$ <span class="token function">wget</span> Spark下载地址
$ <span class="token function">wget</span> https://repo.huaweicloud.com/java/jdk/8u201-b09/jdk-8u201-linux-x64.tar.gz
$ <span class="token function">ls</span>
hadoop-3.3.1.tar.gz  jdk1.8.0_201.tar.gz	scala-2.12.14.tgz	spark-3.1.2-bin-hadoop3.2.tgz

$ <span class="token function">tar</span> -zxvf hadoop-3.3.1.tar.gz
$ <span class="token function">tar</span> -zxvf scala-2.12.14.tgz
$ <span class="token function">tar</span> -zxvf spark-3.1.2-bin-hadoop3.2.tgz
$ <span class="token function">tar</span> -zxvf

$ <span class="token function">ls</span>
hadoop-3.3.1  jdk1.8.0_201  scala-2.12.14  spark-3.1.2-bin-hadoop3.2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><h3 id="配置java-scala环境变量"><a href="#配置java-scala环境变量" class="header-anchor">#</a> 配置Java, Scala环境变量</h3> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">vi</span> /etc/profile

// Java
<span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/opt/jdk1.8.0_201/ 
<span class="token assign-left variable">JAVA_BIN</span><span class="token operator">=</span><span class="token variable">${JAVA_HOME}</span>/bin 
<span class="token assign-left variable">JRE_HOME</span><span class="token operator">=</span><span class="token variable">${JAVA_HOME}</span>/jre <span class="token assign-left variable">CLASSPATH</span><span class="token operator">=</span><span class="token variable">${JRE_HOME}</span>/lib:<span class="token variable">${JAVA_HOME}</span>/lib:<span class="token variable">${JRE_HOME}</span>/lib/charsets.jar 
<span class="token builtin class-name">export</span>  JAVA_HOME  JAVA_BIN JRE_HOME  <span class="token environment constant">PATH</span>  CLASSPATH
// Scala 
<span class="token builtin class-name">export</span> <span class="token assign-left variable">SCALA_HOME</span><span class="token operator">=</span>/opt/scala-2.12.12 
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">$SCALA_HOME</span>/bin

$ <span class="token builtin class-name">source</span> /etc/profile

$ <span class="token function">vi</span> ~/.bashrc

<span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/opt/jdk1.8.0_201 
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token variable">$JAVA_HOME</span>/bin:<span class="token environment constant">$PATH</span> 
<span class="token builtin class-name">export</span> <span class="token assign-left variable">CLASSPATH</span><span class="token operator">=</span>.:<span class="token variable">$JAVA_HOME</span>/lib/dt.jar:<span class="token variable">$JAVA_HOME</span>/lib/tools.jar

$ <span class="token builtin class-name">source</span> ~/.bashrc
$ Java -version
$ scala -version
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><h3 id="配置hadoop"><a href="#配置hadoop" class="header-anchor">#</a> 配置Hadoop</h3> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token builtin class-name">cd</span> /opt/hadoop-3.3.1
$ <span class="token function">vi</span> etc/hadoop/hadoop-env.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>末尾添加jdk目录</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">JAVA_HOME</span><span class="token operator">=</span>/opt/jdk1.8.0_201
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>配置Hadoop环境变量</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">vi</span> ~/.bashrc

<span class="token builtin class-name">export</span> <span class="token assign-left variable">HADOOP_HOME</span><span class="token operator">=</span>/opt/hadoop-3.3.1
<span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span><span class="token builtin class-name">:</span><span class="token variable">${JAVA_HOME}</span>/bin:<span class="token variable">${HADOOP_HOME}</span>/bin:<span class="token variable">${HADOOP_HOME}</span>/sbin

$ hadoop version <span class="token comment"># 验证环境变量生效</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h3 id="安装ssh"><a href="#安装ssh" class="header-anchor">#</a> 安装SSH</h3> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ yum <span class="token function">install</span> openssh-clients
$ yum <span class="token function">install</span> openssh-server
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h3 id="配置免密登陆"><a href="#配置免密登陆" class="header-anchor">#</a> 配置免密登陆</h3> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token builtin class-name">cd</span> ~/.ssh
$ ssh-keygen -t rsa
$ <span class="token function">cat</span> ~/.ssh/id_rsa.pub <span class="token operator">&gt;&gt;</span> ~/.ssh/authorized_keys
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>笔者在测试时ssh localhost报错：ssh: connect to host localhost port 22: Cannot assign requested address；判断ssh服务没开启，但用systemctl开启服务时报错：</p> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>System has not been booted with systemd as init system <span class="token punctuation">(</span>PID <span class="token number">1</span><span class="token punctuation">)</span>. Can't operate.
Failed to connect to bus: Host is down
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>只能手动去开启，分两步：</p> <p>（1）生成 host_key</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key
$ ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key
$ ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -N <span class="token string">&quot;&quot;</span>
$ ssh-keygen -t dsa -f /etc/ssh/ssh_host_ed25519_key
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>（2）启动SSH服务</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ /usr/sbin/sshd -f /etc/ssh/sshd_config
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h3 id="将当前容器打包成镜像"><a href="#将当前容器打包成镜像" class="header-anchor">#</a> 将当前容器打包成镜像</h3> <p>docker commit -m &quot;centos7 with hadoop and ssh&quot; linux1 centos7-hadoop-ssh</p> <h3 id="开启三个容器"><a href="#开启三个容器" class="header-anchor">#</a> 开启三个容器</h3> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ docker run --name hadoop0 --hostname hadoop0 --privileged<span class="token operator">=</span>true -P -p  <span class="token number">50070</span>:50070 -p <span class="token number">8088</span>:8088 -p <span class="token number">9870</span>:9870 -p  <span class="token number">9864</span>:9864 -p <span class="token number">8080</span>:8080 centos7-hadoop-ssh  

$ docker run --name hadoop1 --hostname hadoop1 --privileged<span class="token operator">=</span>true  centos7-hadoop-ssh  

$ docker run --name hadoop2 --hostname hadoop2 --privileged<span class="token operator">=</span>true centos7-hadoop-ssh  
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="host配置主机名和ip地址的映射"><a href="#host配置主机名和ip地址的映射" class="header-anchor">#</a> host配置主机名和IP地址的映射</h3> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">vi</span> /etc/hosts

<span class="token number">172.17</span>.0.4      hadoop2
<span class="token number">172.17</span>.0.2      hadoop1
<span class="token number">172.17</span>.0.3      hadoop0
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>测试过可以互相ping通，且可以互相ssh连接，不难理解为什么，在最开始的容器中，我们已经生成了SSH密钥对，并将公钥放入了authorized_keys中，SSH是通过非对称加密进行的认证，带着公钥进行SSH连接就可以通过认证实现免密登陆，由于我们是先通过容器创建镜像，用同一个镜像又开了三个容器，这样每个容器都有相同的公私钥，自然就可以通过其它容器的认证了。</p> <h3 id="三个容器上配置hadoop"><a href="#三个容器上配置hadoop" class="header-anchor">#</a> 三个容器上配置Hadoop</h3> <p>这里配置内容和Hadoop_env所讲类似，可以参考那篇文章。</p> <p>另外还需要修改$HADOOP_HOME/etc/hadoop/hadoop-env.sh，添加：</p> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code><span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_NAMENODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_DATANODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">HDFS_SECONDARYNAMENODE_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_RESOURCEMANAGER_USER</span><span class="token operator">=</span>root
<span class="token builtin class-name">export</span> <span class="token assign-left variable">YARN_NODEMANAGER_USER</span><span class="token operator">=</span>root
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>在$HADOOP_HOME/etc/hadoop/目录下新建slaves文件，配置DataNode节点，不做此配置DataNode节点无法启动。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">vim</span> etc/hadoop/slaves

hadoop1
hadoop2
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>配置完使用scp将配置分发到Hadoop1和hadoop2上</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">scp</span>  -rq /opt/hadoop-3.3.1   hadoop1:/opt
$ <span class="token function">scp</span>  -rq /opt/hadoop-3.3.1   hadoop2:/opt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>确定之前配置文件中的tmp和namenode、datanode路径都存在，没有则创建；确定可以使用which，没有则yum install -y which。</p> <p>首次启动集群需要在hadoop0格式化namenode</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ bin/hdfs namenode -format
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>hadoop0为namenode节点，hadoop1~3均为datanode节点，三个容器中都启动hdfs。</p> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ sbin/start-dfs.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>每个容器内可以使用 jps 确定一下对应的进程是否启动成功。</p> <p>因为在启动docker的时候做了端口映射，直接localhost:9870就可以访问namenode的web页面：</p> <p><img src="/blog/assets/img/image-20210820194850968.e71e53ea.png" alt="image-20210820194850968"></p> <h3 id="启动spark"><a href="#启动spark" class="header-anchor">#</a> 启动Spark</h3> <p>在master节点的对应容器内进入Spark目录，执行：</p> <div class="language-Bash line-numbers-mode"><pre class="language-bash"><code>$ ./sbin/start-master.sh
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在映射的8080端口查看master节点启动成功：</p> <p><img src="/blog/assets/img/image-20210820201856126.4e35b77f.png" alt="image-20210820201856126"></p> <p>记录下上面的URL，进入Worker节点对应的容器Spark目录下，</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token comment"># ./sbin/start-worker.sh &lt;url&gt;</span>
$ ./sbin/start-worker.sh spark://hadoop0:7077
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>启动成功后在之前Master节点的web页面下可以看到</p> <p><img src="/blog/assets/img/image-20210820202122276.14aa0d08.png" alt="image-20210820202122276"></p> <h3 id="测试spark"><a href="#测试spark" class="header-anchor">#</a> 测试Spark</h3> <p>运行官方样例 —— 计算Pi</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>.bin/spark-submit --class org.apache.spark.examples.SparkPi <span class="token punctuation">\</span>
--master spark://hadoop0:7077 <span class="token punctuation">\</span>
examples/jars/spark-examples_2.12-3.1.2.jar <span class="token punctuation">\</span>
<span class="token number">10</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>查看执行情况：</p> <p><img src="/blog/assets/img/image-20210820210834416.b8f95a9d.png" alt="image-20210820210834416"></p> <p>综上，我们的Docker模拟完全分布式Hadoop+Spark集群就已经搭建完毕，并且通过了测试。</p> <p>之后会一点点进行 Spark 学习的经验分享！</p></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">8/26/2021, 9:07:17 PM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#docker-vs-虚拟机" class="sidebar-link reco-side-docker-vs-虚拟机" data-v-70334359>Docker vs 虚拟机</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#docker-镜像" class="sidebar-link reco-side-docker-镜像" data-v-70334359>Docker 镜像</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#启动容器" class="sidebar-link reco-side-启动容器" data-v-70334359>启动容器</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#配置java-scala环境变量" class="sidebar-link reco-side-配置java-scala环境变量" data-v-70334359>配置Java, Scala环境变量</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#配置hadoop" class="sidebar-link reco-side-配置hadoop" data-v-70334359>配置Hadoop</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#安装ssh" class="sidebar-link reco-side-安装ssh" data-v-70334359>安装SSH</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#配置免密登陆" class="sidebar-link reco-side-配置免密登陆" data-v-70334359>配置免密登陆</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#将当前容器打包成镜像" class="sidebar-link reco-side-将当前容器打包成镜像" data-v-70334359>将当前容器打包成镜像</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#开启三个容器" class="sidebar-link reco-side-开启三个容器" data-v-70334359>开启三个容器</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#host配置主机名和ip地址的映射" class="sidebar-link reco-side-host配置主机名和ip地址的映射" data-v-70334359>host配置主机名和IP地址的映射</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#三个容器上配置hadoop" class="sidebar-link reco-side-三个容器上配置hadoop" data-v-70334359>三个容器上配置Hadoop</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#启动spark" class="sidebar-link reco-side-启动spark" data-v-70334359>启动Spark</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/BigData/Spark/setup_env.html#测试spark" class="sidebar-link reco-side-测试spark" data-v-70334359>测试Spark</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.137d6028.js" defer></script><script src="/blog/assets/js/3.305110ea.js" defer></script><script src="/blog/assets/js/1.3ef5e73d.js" defer></script><script src="/blog/assets/js/7.3005c2a8.js" defer></script>
  </body>
</html>
